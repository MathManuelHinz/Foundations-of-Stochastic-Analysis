

\begin{*definition}%1.3
    A one-dimensional \underline{\textbf{standard}} Brownian motion is a real-valued stoch. process:
    \begin{enumerate}
        \item $B_0=0$
        \item $\forall n\geq 1,0=t_0<t_1<\dots<t_n B_{t_1},B_{t_2}-B_{t_1},\dots,B_{t_n}-B_{t_{n-1}}$ are independent r.v. wth $B_{t_k}-B_{t_{k-1}}\sim \mathcal{N}(0,t_k-t_{k-1})$
        \item for almost all $\omega\in \Omega$\[t\mapsto B_t(\omega)\] are continuous.
    \end{enumerate}
\end{*definition}

\subsection{A review of conditional expectation}

\subsubsection{Definitions}

\begin{definition}\label{con_exp}
    Let $(\Omega,\mathcal{F},\mathbb{P})$ be a prob. space and $\mathcal{G}\subset \mathcal{F}$ is sub-$\sigma$-algebra.
    Let $X\in L^1(\mathbb{P})$. The \underline{\textbf{conditional expectation of $X$ given $\mathcal{G}$}}, $\mathbb{E}(X|\mathcal{G})$ is any random
    variable $Y$ s.t.
    \begin{enumerate}
        \item $Y$ is $\mathcal{G}$-measurable
        \item $\forall A\in \mathcal{G}$, $\int_A Yd\mathbb{P}=\int_A X d\mathbb{P}$
    \end{enumerate}
\end{definition}

\begin{remark}
    If $Y,\tilde{Y}$ satisfy \ref{con_exp} $\implies Y=\tilde{Y}$ a.s..
\end{remark}
\underline{\textbf{In words:}} In $\mathcal{G}$ we have partial information,
for $A\subset \mathcal{G}$ we know wether it occurs or not.

$$\mathbb{E}[X|\mathcal{G}]$$ is the best guess of $X$ given the information in $\mathcal{G}$.

\subsubsection{Examples}

\begin{example}
    Let $X$ be $\mathcal{G}$ measurable, then $\mathbb{E}(X|\mathcal{G})=X$ a.s.
\end{example}

\begin{example}
    Let $X$ be independent of $\mathcal{G}\implies \mathbb{E|\mathcal{G}}=\mathbb{E}(X)$ a.s..
\end{example}

\begin{remark}
    $\forall B\in \mathcal{B}(\R), A\in\mathcal{G}, \mathbb{P}(X\in B\cap A)=\mathbb{P}(X\in B)\mathbb{P}(A)$.
    
    $Y\coloneqq \mathbb{E}(X)\implies$ $Y$ $\mathcal{G}$ measurable. Let $A\in\mathcal{G}$:
    \[\int_A \mathbb{E}(X)d\mathbb{P}=\mathbb{E}(X)\mathbb{P}(A)=\mathbb{E}(X)\mathbb{E}(1_A)\]
    \[\stackrel{X,1_A \text{ in dependent}}{=}\mathbb{E}(X1_A)=\int_A X d\mathbb{P}\]  
\end{remark}

\begin{example}
    Assume $\Omega=\bigcup_{k\geq 1}\Omega_k$ disjoint union with $\mathbb{P}(\Omega_k)>0$.
    Let $\mathcal{G}=\sigma(\Omega_1,\dots,\Omega_n)$
    \begin{align*}
       \implies \forall k \geq 1 Y\coloneqq \mathbb{E}(X|\mathcal{G})=\frac{\mathbb{E}(X1_{\Omega_k})}{\mathbb{P}(\Omega_k)}\text{ on }\Omega_k
    \end{align*}
    If $\omega\in\Omega_k$ what is the best guess of $X(\omega)$. $Y$ is constant on $\Omega_k\implies Y$ is measurable.
    Since $\mathcal{G}$ is generated by the $\Omega_k$ (smallest elements), we have to verify the second property only for $A=\Omega_k\forall k$.
\end{example}

\begin{remark}
    If $\mathcal{G}=\{\emptyset,\Omega\}\implies$ \[\mathbb{E}(X|\mathcal{G})=\mathbb{E}(X) \text{ a.s.}\]
\end{remark}

\begin{proposition}
$X\in L^1(\mathbb{P})$
\begin{enumerate}
    \item $\mathbb{E}( \mathbb{E}(X|\mathcal{G}))=\mathbb{E}(X)$
    \item If $\mathcal{G}_1\subset \mathcal{G}_2$ are two sub-$\sigma$-algebras,
    \[\implies \mathbb{E}(\mathbb{E}(X|\mathcal{G}_1)|\mathcal{G}_2)=\mathbb{E(X|\mathcal{G}_1)}\text{ a.s.}\]
    \[\implies \mathbb{E}(\mathbb{E}(X|\mathcal{G}_2)|\mathcal{G}_1)=\mathbb{E(X|\mathcal{G}_1)}\text{ a.s.}\]
    \item If $X$ is $\mathcal{G}$-measurable, $Y$ a random variable, $\mathbb{E}(|Y|)<\infty,\mathbb{E}(|XY|)<\infty$\[\implies \mathbb{E}(XY|\mathcal{G})=X\mathbb{E}(Y|\mathcal{G})\text{ a.s.}\] 
\end{enumerate}    
\end{proposition}


\subsubsection{Geometric interpretation}

\begin{proposition}
    Let $X$ be a r.v. with $\mathbb{E}(|X|^2)<\infty$
    $\implies \mathbb{E}(X|\mathcal{G})$ is the random variable $Y$ which is $\mathcal{G}$-measurable and minimizes
    \[\mathbb{E}((X-Y)^2)\]
\end{proposition}

\underline{\textbf{Notation}}

\[L^2(\mathcal{G})=\{Y \mathcal{G}\text{-measurable}|\mathbb{E}(|Y|^2)<\infty\}\]


\begin{proof}
    $L^2(\mathcal{F})$ is Hilbert space and $L^2(\mathcal{G})$ is a closed subspace.

    If $Z\in L^2(\mathcal{G}) \implies \mathbb{E}(Z\cdot \mathbb{E}(X|\mathcal{G}))=\mathbb{E}(\mathbb{E}(ZX|\mathcal{G}))=\mathbb{E}(ZX)$

    \[\implies \mathbb{E}(Z(X-\mathbb{E}(X|\mathcal{G})))=0\]

    If $Y \in L^2(\mathcal{G})$ and $Y=\mathbb{E}(X|\mathcal{G})+Z$
    \[\implies \mathbb{E}(|X-Y|^2)=\mathbb{E}((X-\mathbb{E}(X|\mathcal{G})-Z)^2)\]
    \[=\mathbb{E}((X-\mathbb{E}(X|\mathcal{G}))^2)+\mathbb{E}(Z^2)-\underbrace{2\mathbb{E(Z(X-\mathbb{E}(X|\mathcal{G})))}}_{=0}\]
    $\implies$ Minimize $\mathbb{E}((X-Y)^2) \iff \mathbb{E}(Z^2)=0\implies Z=0$

\end{proof}

\subsubsection{Random walk}
Let $X_1,\dots,X_n$ iid random variables with $\mathbb{E}(X_i)=0$; let $S_n\coloneqq X_1+\dots+X_n$.
Let $\mathcal{G}_n=\sigma(X_1,\dots,X_n),n\geq 1$
\[\implies \mathcal{G}_1\subset \mathcal{G}_2\subset \dots\]
\[\implies \forall m\leq n: \exists\mathcal{E}(S_n|\mathcal{G}_n)=S_m \text{a.s.}\]

\underline{\textbf{Ideed:}} $Y=S_n$ is $\mathcal{G}_m$-measurable.

Let $A\in\mathcal{G}_m\implies \int_A S_m d\mathcal{P}=\int_A S_n d\mathcal{P}$

But $\int_A S_nd\mathbb{P}=\int_A S_m d\mathbb{P}+\sum_{k=1}^n \underbrace{\int_A X_k d\mathbb{P}}_{\mathbb{E}(X_k1_A)=\mathbb{E}(X_k)\mathbb{1_A}=0}$

and $X_k, 1_A$ are independent by assumption.

\subsubsection{Conditional densities}

\begin{definition}
    Let $X,Y$ be r. v. with $\mathbb{E}(|X|)<\infty$.
    
    $\implies$ We define $\mathbb{E}(X|Y)\coloneqq \mathbb{E}(X|\sigma(Y))$
\end{definition}

\begin{itemize}
    \item Consider the case where $(X,Y)$ have a density with respect to Lebesgue, i.e.
    \[\forall B\in\mathcal{B}(\R^2)\mathbb{P}((X,Y)\in B)=\int_B f(x,y)dxdy\]
\end{itemize}

\begin{remark}
    $\mathbb{P}(A|\mathcal{G})\coloneqq \mathbb{E}(1_A|\mathcal{G})$.
\end{remark}

Assume $\int f(x,y) dx >0 \forall y$

\begin{proposition}
    Let $X$ be a r.v. and $g$ a function s.t. $\mathbb{E}(|g(X)|)<\infty$.
    \[\implies \mathbb{E}(g(X)|Y)=h(Y)\]
    where \[h(Y)=\frac{\int g(x) f(x,y) dx}{\int f(x,y)dx}\]
\end{proposition}

\begin{proof}
    How to derive?
    \begin{align*}
        \mathbb{P}(X=x|Y=y)=\frac{\mathbb{P}(X=x\cap Y=y)}{\mathbb{P}(Y=y)}=\frac{f(x,y)}{\int f(x,y)dx}\\
        \implies \mathbb{E}(g(X)|Y=y)=\frac{\int g(x)f(x,y)dx}{\int f(x,y)dx}=h(y)
    \end{align*}    

    $h(Y)$ is $\mathcal{G}$-measurable.

    For $A\in \sigma(Y)\implies \exists \tilde{A}\in\mathcal{B}(\R)$ s.t. $A=\{\omega:Y(\omega)=\tilde{A}\}$
    \[\implies \int_A h(y)dy=\int_{\tilde{A}} dy \underbrace{\int dx h(y f(x,y))}_{=\int dx g(x) f(x,y)}\]
    \[\implies \int dx g(x)\int_{\tilde{A}} dy f(x,y)=\mathbb{E}(g(X)1_{\tilde{A}}(Y))=\mathbb{E}(g(X)1_A)\]

\end{proof}


